---
title: "Class_Balance_Models"
author: "James A. Coppock"
date: "10/28/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

In this document we are compiling all of our models for the class imbalance paradigm.

```{r cars}
library(tidyverse)
library(ggplot2)
library(lubridate)
library(patchwork)
library(gridExtra)
library(psych)
library(corrplot)
library(ggfortify)
library(factoextra)
library(class) #knn
library(gmodels) # CrossTable()
library(caret) # creatFolds()
library(caTools) #sample.split()
library(ROCR) # prediction(), performance()
library(MLeval)
library(MLmetrics)
library(sparseLDA)
library(kernlab)
library(stepPlr)
library(ROCit)
library(plyr)
library(e1071)
library(glmnet)
library(randomForest)
set.seed(123)
```

## Load Test/Train Datasets
set all targets to factor | Index all other columns for scaling

```{r pressure, echo=FALSE}
set.seed(123)
train_df <- read_csv("Train_df_V2.csv") # this is actually overall dataframe
test_df <- read_csv("Test_df_V2.csv") # this is actually overall dataframe
nFallsTrain<-sum(train_df$targets)
nFallsTest<-sum(test_df$targets)
train_df$Target<-as.factor(train_df$Target)
test_df$Target<-as.factor(test_df$Target)
nums<-unlist(lapply(train_df, is.numeric))
cat('\nThe training DF is', as.character(dim(train_df)[1]),'instances long\n')
cat('The training DF is', as.character(dim(test_df)[1]),'instances long')

drops <- c("X1")
train_df<-subset(train_df,select = -c(X1))
test_df<-subset(test_df,select = -c(X1))
```
```{r}
set.seed(123)
Falls_train_df<-train_df[train_df$Target==1,]
noFalls_train_df<-train_df[train_df$Target==0,]
inds<-seq(from=1,by=1,to=dim(noFalls_train_df)[1])
noFall_ind<-sample(inds,size=dim(Falls_train_df)[1])
noFalls_train_df<-noFalls_train_df[noFall_ind,]
train_df<-rbind(noFalls_train_df,Falls_train_df)
```

```{r}
set.seed(123)
Falls_test_df<-test_df[test_df$Target==1,]
noFalls_test_df<-test_df[test_df$Target==0,]
inds<-seq(from=1,by=1,to=dim(noFalls_test_df)[1])
noFall_ind<-sample(inds,size=dim(Falls_test_df)[1])
noFalls_test_df<-noFalls_test_df[noFall_ind,]
test_df<-rbind(noFalls_test_df,Falls_test_df)
```


```{r}
set.seed(123)
train_df[, c(2):c(ncol(train_df))] <- scale(train_df[, c(2):c(ncol(train_df))], center=TRUE, scale=TRUE)
test_df[, c(2):c(ncol(test_df))] <- scale(test_df[, c(2):c(ncol(test_df))], center=TRUE, scale=TRUE)

```


```{r}
set.seed(123)
trainResults <- function(model, architecture){
  for_lift <- data.frame(Class = model$pred$obs, rf = model$pred$R, resample = model$pred$Resample)
  lift_df <-  data.frame()
  for (fold in unique(for_lift$resample)) {
    fold_df <- dplyr::filter(for_lift, resample == fold)
    lift_obj_data <- lift(Class ~ rf, data = fold_df, class = "R")$data
    lift_obj_data$fold = fold
    lift_df = rbind(lift_df, lift_obj_data)
  }
  lift_obj <- lift(Class ~ rf, data = for_lift, class = "R")
  library(plyr)
  accuracy <- ddply(model$pred, "Resample", summarise,
        accuracy = Accuracy(pred, obs))
  res <- evalm(list(model_lr),gnames=c(architecture))  
  return(accuracy)
}

errorRate <- function(model, test){
  1-mean(model==test)
}

```


## Build LR Here

```{r}
set.seed(123)
train_x <- model.matrix(Target~., train_df)
train_y <- train_df$Target
test_x <- model.matrix(Target~., test_df)
test_y <- test_df$Target
model_rr <- cv.glmnet(train_x, train_y, alpha=0, lambda=10^seq(-5,5, length=200), 
                         type.measure="class", nfolds=10, family="binomial")
plot(model_rr)

```
```{r}
set.seed(123)

model_rr$lambda.min
```


```{r}
set.seed(123)
coef(model_rr, s = model_rr$lambda.min)

```

```{r}
set.seed(123)
train_perf_rr <- predict(model_rr, s = model_rr$lambda.min, 
                         newx = train_x, type = "response")
ROC_train_rr <- rocit(score=as.vector(train_perf_rr), class=train_y)
plot(ROC_train_rr, legend = TRUE, YIndex = FALSE, values = TRUE)
summary(ROC_train_rr)

rr_train_binary <- ifelse(train_perf_rr>0.5, 1, 0)
confusionMatrix(table(rr_train_binary, train_df$Target), positive="1")

```


```{r}
set.seed(123)
test_perf_rr <- predict(model_rr, s = model_rr$lambda.min, 
                         newx = test_x, type = "response")
ROC_test_rr <- rocit(score=as.vector(test_perf_rr), class=test_y)
plot(ROC_test_rr, legend = TRUE, YIndex = FALSE, values = TRUE)
summary(ROC_test_rr)

rr_test_binary <- ifelse(test_perf_rr>0.5, 1, 0)
confusionMatrix(table(rr_test_binary, test_df$Target), positive="1")
```

```{r}
set.seed(123)
rr_train_error <- errorRate(rr_train_binary, train_y)
cat("\nTrain accuracy:   ", 1-rr_train_error,
    "\nTrain error rate: ", rr_train_error,
    "\nTrain AUC:        ", ROC_train_rr$AUC)

rr_test_error <- errorRate(rr_test_binary, test_y)
cat("\nTest accuracy:    ", 1-rr_test_error,
    "\nTest error rate:  ", rr_test_error,
    "\nTest AUC:         ", ROC_test_rr$AUC)

```

## Build SVM

```{r}
set.seed(123)
model_svm <- tune.svm(Target~., data = train_df, 
                      type = 'C-classification',
                      cost = 10^seq(-3,3, length=10),
                      kernel="polynomial",degree=c(2,3,4,5),
                      # kernel = "linear",tunecontrol=tune.control(cross=5),
                      probability=TRUE)
summary(model_svm)
plot(model_svm)

```

```{r}
set.seed(123)
model_svm$performances
train_perf_svm <- predict(model_svm$best.model, train_df[,2:ncol(train_df)], probability=TRUE)
predictions_train_svm <- data.frame(attr(train_perf_svm, "probabilities"))[,2]
ROC_train_svm <- rocit(score=as.vector(predictions_train_svm), class=train_y)
plot(ROC_train_svm, legend = TRUE, YIndex = FALSE, values = TRUE)
summary(ROC_train_svm)

svm_train_binary <- ifelse(predictions_train_svm>0.5, 1, 0)
confusionMatrix(table(svm_train_binary, train_df$Target), positive="1")

```

```{r}
set.seed(123)
test_perf_svm <- predict(model_svm$best.model, test_df[,2:ncol(test_df)], probability=TRUE)
predictions_test_svm <- data.frame(attr(test_perf_svm, "probabilities"))[,2]
ROC_test_svm <- rocit(score=as.vector(predictions_test_svm), class=test_y)
plot(ROC_test_svm, legend = TRUE, YIndex = TRUE, values = TRUE)
summary(ROC_test_svm)
#optimal cutpoints package

svm_test_binary <- ifelse(predictions_test_svm>0.5, 1, 0)
confusionMatrix(table(svm_test_binary, test_df$Target), positive="1")
```

```{r}
set.seed(123)
svm_train_error <- errorRate(svm_train_binary, train_y)
cat("\nTrain accuracy:   ", 1-svm_train_error,
    "\nTrain error rate: ", svm_train_error,
    "\nTrain AUC:        ", ROC_train_svm$AUC)

svm_test_error <- errorRate(svm_test_binary, test_y)
cat("\nTest accuracy:    ", 1-svm_test_error,
    "\nTest error rate:  ", svm_test_error,
    "\nTest AUC:         ", ROC_test_svm$AUC)

```

## Build Random Forest

```{r}
set.seed(123)
model_rf <- tune.randomForest(Target~., data = train_df, 
                      mtry = sqrt(ncol(train_df)-1),
                      tunecontrol=tune.control(cross=5),
                      ntree = c(1,10,100,1000,5000), probability=TRUE, 
                      importance=TRUE)
summary(model_rf)

```

```{r}
set.seed(123)
model_rf$performances
plot(model_rf$performances[,2],model_rf$performances[,3],xlab="ntree", ylab="Error", log="x")
model_rf$best.model

```

```{r}
set.seed(123)
model_rf$performances
train_perf_rf <- predict(model_rf$best.model, train_df[,2:ncol(train_df)], type="prob")
predictions_train_rf <- data.frame(train_perf_rf)[,2]
ROC_train_rf <- rocit(score=as.vector(predictions_train_rf), class=train_y)
plot(ROC_train_rf, legend = TRUE, YIndex = FALSE, values = TRUE)
summary(ROC_train_rf)

rf_train_binary <- ifelse(predictions_train_rf>0.5, 1, 0)
confusionMatrix(table(rf_train_binary, train_df$Target), positive="1")

```

```{r}
set.seed(123)
test_perf_rf <- predict(model_rf$best.model, test_df[,2:ncol(test_df)], type="prob")
predictions_test_rf <- data.frame(test_perf_rf)[,2]
ROC_test_rf <- rocit(score=as.vector(predictions_test_rf), class=test_y)
plot(ROC_test_rf, legend = TRUE, YIndex = FALSE, values = TRUE)
summary(ROC_test_rf)

rf_test_binary <- ifelse(predictions_test_rf>0.5, 1, 0)
confusionMatrix(table(rf_test_binary, test_df$Target), positive="1")
```

```{r}
set.seed(123)
rf_train_error <- errorRate(rf_train_binary, train_y)
cat("\nTrain accuracy:   ", 1-rf_train_error,
    "\nTrain error rate: ", rf_train_error,
    "\nTrain AUC:        ", ROC_train_rf$AUC)

rf_test_error <- errorRate(rf_test_binary, test_y)
cat("\nTest accuracy:    ", 1-rf_test_error,
    "\nTest error rate:  ", rf_test_error,
    "\nTest AUC:         ", ROC_test_rf$AUC)

```

## Plot Combined ROC Curve

```{r}
set.seed(123)
par(pty="s")
plot(ROC_test_rr$FPR, ROC_test_rr$TPR, type = "l", 
     xlab = "1-Specificity", ylab = "Sensitivity", col = "blue",
     main = "Naive Approach - ROC Curves")
lines(ROC_test_rf$FPR, ROC_test_rf$TPR, type = "l", col = "red")
lines(ROC_test_svm$FPR, ROC_test_svm$TPR, type = "l", col = "green")
abline(coef = c(0,1))
LR_char<-paste("LR:",as.character(round(ROC_test_rr$AUC,3)))
SVM_char<-paste("SVM:",as.character(round(ROC_test_svm$AUC,3)))
RF_char<-paste("RF:",as.character(round(ROC_test_rf$AUC,3)))
legend("bottomright",c(LR_char,RF_char, SVM_char), lty="solid", col=c("blue","red", "green"),cex=1,
        title="Model           AUC")
```


## Ignore models below

```{r}

k = 5
myControl_lr <- trainControl(
                             method = "repeatedcv", number = k,
                             summaryFunction = twoClassSummary,
                             classProbs = TRUE,
                             verboseIter = FALSE,
                             savePredictions = TRUE,
                             allowParallel = FALSE
                            )
myGrid_lr <-  expand.grid(.lambda=1e-5, 
                         .cp="bic")
model_lr <- train(Target ~., 
                 data = train_df, 
                 method = "plr",
                 tuneGrid = myGrid_lr, 
                 metric = "ROC",
                 trControl = myControl_lr,
                 preProcess = c("center", "scale"))

model_lr

```

See model summary

```{r}

summary(model_lr)
max((model_lr$results)$ROC)

```

See tuning results

```{r}

plot(model_lr)

trellis.par.set(caretTheme())
densityplot(model_lr, pch = "|")

```


```{r}

lr_train_results <- trainResults(model_lr, "Logistic Regression")
lr_train_results
cat("5-fold train accuracy: ", mean(lr_train_results[,2]))

```

```{r}
library(ROCit)
prediction_lr <- predict(model_lr, test_df, type = "prob")
ROCit_lr <- rocit(score=prediction_lr[,2],class=test_df$Target)
plot(ROCit_lr, legend = TRUE, YIndex = FALSE, values = TRUE)
summary(ROCit_lr)

lr_binary <- ifelse(prediction_lr[,2]>0.5, 1, 0)
lr_error <- errorRate(lr_binary, test_df$Target)
cat("\nTest accuracy:   ", 1-lr_error,
    "\nTest error rate: ", lr_error,
    "\nTest AUC:        ", ROCit_lr$AUC)

confusionMatrix(table(lr_binary, test_df$Target), positive="1")
```

## Build LDA Here

```{r}

k = 5
myControl_lda <- trainControl(
                             method = "repeatedcv", number = k,
                             summaryFunction = twoClassSummary,
                             classProbs = TRUE,
                             verboseIter = FALSE,
                             savePredictions = TRUE,
                             allowParallel = TRUE
                            )
myGrid_lda <-  expand.grid(.NumVars = c(2:50), 
                           .lambda = c(0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000)
                           )
model_lda <- train(Target ~., 
                 data = train_df, 
                 method = "sparseLDA",
                 tuneGrid = myGrid_lda, 
                 metric = "ROC",
                 trControl = myControl_lda,
                 preProcess = c("center", "scale"),
                 verbose = FALSE)

model_lda

```

See model summary

```{r}

summary(model_lda)
max((model_lda$results)$ROC)

```

See tuning results

```{r}

plot(model_lda)

trellis.par.set(caretTheme())
densityplot(model_lda, pch = "|")

trellis.par.set(caretTheme())
plot(model_lda, metric = "ROC", plotType = "level",
     scales = list(x = list(rot = 90)))

```

```{r}

lda_train_results <- trainResults(model_lda, "LDA")
lda_train_results
cat("5-fold train accuracy: ", mean(lda_train_results[,2]))

```

```{r}
library(ROCit)
prediction_lda <- predict(model_lda, test_df, type = "prob")
ROCit_lda <- rocit(score=prediction_lda[,2],class=test_df$Target)
plot(ROCit_lda, legend = TRUE, YIndex = FALSE, values = TRUE)
summary(ROCit_lda)

lda_binary <- ifelse(prediction_lda[,2]>0.5, 1, 0)
lda_error <- errorRate(lda_binary, test_df$Target)
cat("\nTest accuracy:   ", 1-lda_error,
    "\nTest error rate: ", lda_error,
    "\nTest AUC:        ", ROCit_lda$AUC)

confusionMatrix(table(lda_binary, test_df$Target), positive="1")
```

## Build SVM Here

```{r}

k = 5
myControl_svm <- trainControl(
                             method = "repeatedcv", number = k,
                             summaryFunction = twoClassSummary,
                             classProbs = TRUE,
                             verboseIter = FALSE,
                             savePredictions = TRUE
                            )
myGrid_svm <- expand.grid(                    
                          C = c(0.25, 0.5, 0.75),
                          degree= c(2,3,4),
                          scale = c(0.001, 0.01, 0.1)
                        )
  
model_svm <- train(Target ~., 
                   data = train_df, 
                   method = "svmPoly", 
                   tuneGrid = myGrid_svm, 
                   metric = "ROC",
                   trControl = myControl_svm,
                   preProcess = c("center", "scale"),
                   verbose = FALSE
              )
model_svm

```

See model summary

```{r}

summary(model_svm)
max((model_svm$results)$ROC)

```

See tuning results

```{r}

plot(model_svm)

trellis.par.set(caretTheme())
densityplot(model_svm, pch = "|")

```


```{r}

svm_train_results <- trainResults(model_svm, "SVM")
svm_train_results
cat("5-fold train accuracy: ", mean(svm_train_results[,2]))

```

```{r}
library(ROCit)
prediction_svm <- predict(model_svm, test_df, type = "prob")
ROCit_svm <- rocit(score=prediction_svm[,2],class=test_df$Target)
plot(ROCit_svm, legend = TRUE, YIndex = FALSE, values = TRUE)
summary(ROCit_svm)

svm_binary <- ifelse(prediction_svm[,2]>0.5, 1, 0)
svm_error <- errorRate(svm_binary, test_df$Target)
cat("\nTest accuracy:   ", 1-svm_error,
    "\nTest error rate: ", svm_error,
    "\nTest AUC:        ", ROCit_svm$AUC)

confusionMatrix(table(svm_binary, test_df$Target), positive="1")
```

## Build RF Here

```{r}

customRF <- list(type = "Classification",
                 library = "randomForest",
                 loop = NULL)

customRF$parameters <- data.frame(parameter = c("mtry", "ntree"),
                                  class = rep("numeric", 2),
                                  label = c("mtry", "ntree"))

customRF$grid <- function(x, y, len = NULL, search = "grid") {}

customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs) {
  randomForest(x, y,
               mtry = param$mtry,
               ntree=param$ntree)
}

#Predict label
customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
   predict(modelFit, newdata)

#Predict prob
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
   predict(modelFit, newdata, type = "prob")

customRF$sort <- function(x) x[order(x[,1]),]
customRF$levels <- function(x) x$classes

```

```{r}

k = 5

library(randomForest)
library(mlbench)
library(e1071)

mtry <- sqrt(ncol(train_df)-1)
myControl_rf <- trainControl(
                             method = "repeatedcv", number = k,
                             summaryFunction = twoClassSummary,
                             classProbs = TRUE,
                             verboseIter = FALSE,
                             savePredictions = TRUE
                            )
myGrid_rf <- expand.grid(.mtry=c(10:15), .ntree=c(1000,1500,2000,2500))
model_rf <- train(Target ~., 
                 data = train_df, 
                 method = customRF,
                 tuneGrid = myGrid_rf, 
                 metric = "ROC",
                 trControl = myControl_rf,
                 preProcess = c("center", "scale"))

model_rf

```

See model summary

```{r}

summary(model_rf)
max((model_rf$results)$ROC)

```

See tuning results

```{r}

plot(model_rf)

trellis.par.set(caretTheme())
plot(model_rf, metric = "ROC", plotType = "level",
     scales = list(x = list(rot = 90)))

```


```{r}

rf_train_results <- trainResults(model_rf, "Random Forest")
rf_train_results
cat("5-fold train accuracy: ", mean(rf_train_results[,2]))

```

```{r}
library(ROCit)
prediction_rf <- predict(model_rf, test_df, type = "prob")
ROCit_rf <- rocit(score=prediction_rf[,2],class=test_df$Target)
plot(ROCit_rf, legend = TRUE, YIndex = FALSE, values = TRUE)
summary(ROCit_rf)

rf_binary <- ifelse(prediction_rf[,2]>0.5, 1, 0)
rf_error <- errorRate(rf_binary, test_df$Target)
cat("\nTest accuracy:   ", 1-rf_error,
    "\nTest error rate: ", rf_error,
    "\nTest AUC:        ", ROCit_rf$AUC)

confusionMatrix(table(rf_binary, test_df$Target), positive="1")
```


## Compare Models

```{r}

resamps <- resamples(list(LR = model_lr,
                          LDA = model_lda,
                          SVM = model_svm,
                          RF = model_rf))
summary(resamps)

```

```{r}

theme1 <- trellis.par.get()
theme1$plot.symbol$col = rgb(.2, .2, .2, .4)
theme1$plot.symbol$pch = 16
theme1$plot.line$col = rgb(1, 0, 0, .7)
theme1$plot.line$lwd <- 2
trellis.par.set(theme1)
bwplot(resamps, layout = c(3, 1))

```

```{r}

trellis.par.set(caretTheme())
dotplot(resamps, metric = "ROC")

```

```{r}

trellis.par.set(theme1)
xyplot(resamps, what = "BlandAltman")

```

```{r}

splom(resamps)

```
