---
title: "PCA_and_Models"
author: "Nicole Zimmer"
date: "10/22/2020"
output: html_document
---

```{r message=FALSE}
library(tidyverse)
library(ggplot2)
library(lubridate)
library(patchwork)
library(gridExtra)
library(psych)
library(corrplot)
library(ggfortify)
library(factoextra)
library(class) #knn
library(gmodels) # CrossTable()
library(caret) # creatFolds()
library(caTools) #sample.split()
library(ROCR) # prediction(), performance()
library(MLeval)
library(MLmetrics)
library(sparseLDA)
set.seed(123)
```

Import time features (remove later with train_df)
PCA to find top contributors
Standardize/scale dataset before PCA

```{r}
df <- read_csv("time_features.csv")
targets <- df$Targets
standardized = scale(df[,2:ncol(df)], center=TRUE, scale=TRUE)
standardized = cbind(standardized, targets)
df_standard = as_tibble(standardized)
#standardized_df = standardized_df %>%
#  mutate(target = V1) %>%
#  select(-V1)
#summary(standardized_df)
#head(standardized_df)

sample <- sample.split(df_standard$mean_Falls_df.accX,SplitRatio = 0.8)
train_ti <- subset(df_standard,sample==TRUE)
train_ti <- train_ti[-c(1), ]
test_ti <- subset(df_standard, sample==FALSE)
test_ti <- test_ti[-c(1, 2, 3), ]
train_fq <- read_csv("Train_feat_df.csv")
test_fq <- read_csv("Test_feat_df.csv")

```

Calculate Principal Components and plot screeplot
```{r}
prin_comp = princomp(~ ., df_standard[1:ncol(df_standard)-1], cor = TRUE)
summary(prin_comp)
screeplot(prin_comp,  npcs = 10, type = c("lines"), main="Scree Plot")
```


Plot to find features corresponding to 95% of variance
```{r}
pve = prin_comp$sdev^2/sum(prin_comp$sdev^2)
cum_pve = cumsum(prin_comp$sdev^2)/sum(prin_comp$sdev^2)
comps = 1:72
pve_df = data.frame(comps, pve)
cum_pve_df = data.frame(comps, cum_pve)

ggplot(cum_pve_df, aes(x=comps, y = cum_pve))+geom_line()+geom_point()+labs(x="Principal Component", y = "Cumulative Proportion of Variance Explained", title = "Cumulative Proportion of Variance Explained Over Number of Principal Components")+geom_hline(yintercept = 0.95, color = "red")+geom_text(aes(0, 0.95, label = 0.95, vjust = 1))

cat("Number of principal components needed to describe at least 95% of variance:", min(which(cum_pve > 0.95)))

```
```{r}
prin_comp$loadings
```

BiPlot
```{r, fig.width=12, fig.height=18}
autoplot(prin_comp, data = df_standard, colour = 'targets', loadings = TRUE, loadings.colour = "blue", loadings.label = TRUE)+labs(x="Principal Component 1", y="Principal Component 2", title= "PCA Biplot")
```

```{r}

varPCA <- function(prin_comp, x){
  names(prin_comp$loadings[,x][order(abs(prin_comp$loadings[,x]),decreasing=TRUE)][x])
}

for(i in 1:25){
  cat("\nVariable corresponding to PC", i, ":  ", varPCA(prin_comp, i))
}

```

```{r, fig.width=8, fig.height=8}

pca_df_processed <- data.frame(targets, prin_comp$scores[,1], prin_comp$scores[,2], prin_comp$scores[,3], prin_comp$scores[,4])
head(pca_df_processed)
pca_df_processed[, "targets"] <- sapply(pca_df_processed[, "targets"], as.factor)
colnames(pca_df_processed)[2] <- "PC1" 
colnames(pca_df_processed)[3] <- "PC2" 
colnames(pca_df_processed)[4] <- "PC3" 
colnames(pca_df_processed)[5] <- "PC4" 

a <- ggplot(pca_df_processed, aes(x=PC1, y=PC2, color=targets)) + geom_point() + 
  xlab("PC1 (44.27%)") + ylab("PC2") + scale_color_manual(name = "Fall", labels = c("No", "Yes"), values = c("#4477AA", "#BB4444"))
b <- ggplot(pca_df_processed, aes(x=PC1, y=PC3, color=targets)) + geom_point() + 
  xlab("PC1 (44.27%)") + ylab("PC3") + scale_color_manual(name = "Fall", labels = c("No", "Yes"), values = c("#4477AA", "#BB4444"))
c <- ggplot(pca_df_processed, aes(x=PC1, y=PC4, color=targets)) + geom_point() + 
  xlab("PC1 (44.27%)") + ylab("PC4") + scale_color_manual(name = "Fall", labels = c("No", "Yes"), values = c("#4477AA", "#BB4444"))
d <- ggplot(pca_df_processed, aes(x=PC2, y=PC3, color=targets)) + geom_point() + 
  xlab("PC2 (18.97%)") + ylab("PC3") + scale_color_manual(name = "Fall", labels = c("No", "Yes"), values = c("#4477AA", "#BB4444"))
e <- ggplot(pca_df_processed, aes(x=PC2, y=PC4, color=targets)) + geom_point() + 
  xlab("PC2 (18.97%)") + ylab("PC4") + scale_color_manual(name = "Fall", labels = c("No", "Yes"), values = c("#4477AA", "#BB4444"))
f <- ggplot(pca_df_processed, aes(x=PC3, y=PC4, color=targets)) + geom_point() + 
  xlab("PC3 (9.39%)") + ylab("PC4") + scale_color_manual(name = "Fall", labels = c("No", "Yes"), values = c("#4477AA", "#BB4444"))

a + b + c + d + e + f  + plot_layout(ncol = 2, nrow = 3, guides = "collect") + 
  plot_annotation(
  title = 'Scatterplots Comparing the First 4 Principal Components')


```

Model Building

```{r}

pca_df <- data.frame(targets)

for(i in 1:6){
  pca_df <- cbind(pca_df, prin_comp$scores[,i])
}
#colnames(pca_df) <- c("targets", "PC1", "PC2", "PC3", "PC4", "PC5", "PC6", "PC7", 
#                      "PC8", "PC9", "PC10", "PC11", "PC12", "PC13", "PC14", "PC15",
#                      "PC16", "PC17", "PC18", "PC19", "PC20", "PC21", "PC22", "PC23",
#                      "PC24", "PC25")
colnames(pca_df) <- c("targets", "PC1", "PC2", "PC3", "PC4", "PC5", "PC6")

sample_pca <- sample.split(pca_df$targets,SplitRatio = 0.8)
train_df_pca <- subset(pca_df,sample ==TRUE)
test_df_pca <- subset(pca_df, sample==FALSE)

```


```{r}

errorRate <- function(model, test){
  1-mean(model==test)
}

convertFactor <- function(df){
  df[, "targets"] <- sapply(df[, "targets"], as.factor)
}
  

```


```{r}

build_LR_CV <- function(k, train_df, test_df){
  set.seed(123)
  train_df$targets[train_df$targets == 1] <- "yes"
  train_df$targets[train_df$targets == 0] <- "no"
  convertFactor(train_df)
  convertFactor(test_df)
  train_control <- trainControl(method = "cv",
                                number = k,
                                savePredictions = TRUE, 
                                classProbs = TRUE, 
                                verboseIter = TRUE,
                                summaryFunction = prSummary)
  model <- train(targets ~ .,
                 data = train_df,
                 trControl = train_control,
                 method = "glmnet",
                 family = "binomial",
                 metric = "AUC",
                 tuneGrid=expand.grid(alpha = 0:1, lambda = seq(0.0001, 1, length = 20)))
  plot(model)
  #res <- evalm(model)
  #model$roc
}

build_LR_CV(5, train_ti, test_ti)

```

```{r}

build_LR_CV2 <- function(k, train_df, test_df){
  train_df$targets[train_df$targets == 1] <- "yes"
  train_df$targets[train_df$targets == 0] <- "no"
  myControl <- trainControl(
                             method = "repeatedcv", number = k,
                             summaryFunction = twoClassSummary,
                             classProbs = TRUE,
                             verboseIter = TRUE
                            )
  myGrid <- expand.grid(
                         alpha = c(0:1),
                         lambda = c(0.0001, 0.001, 0.01, 0.1, 1)
                        )
  
  set.seed(33)
  model <- train(targets ~., 
                 data = train_df, 
                 method = "glmnet",
                 tuneGrid = myGrid, 
                 metric = "ROC",
                 trControl = myControl)
  
  #Check the model
  model
  max((model$results)$ROC)
  plot(model) 
  #res <- evalm(model)
  #model$roc
}

build_LR_CV2(10, train_ti, test_ti)

```

```{r}
library(MASS)
build_LDA_CV2 <- function(k, train_df, test_df){
  train_df$targets[train_df$targets == 1] <- "yes"
  train_df$targets[train_df$targets == 0] <- "no"
  myControl <- trainControl(
                             method = "repeatedcv", number = k,
                             summaryFunction = twoClassSummary,
                             classProbs = TRUE,
                             verboseIter = TRUE
                            )
  myGrid <-  expand.grid(.NumVars = c(2:10), 
                        .lambda = c(0, 0.01, 0.1, 1, 10, 100))
  set.seed(33)
  model <- train(targets ~., 
                 data = train_df, 
                 method = "sparseLDA",
                 tuneGrid = myGrid, 
                 metric = "ROC",
                 trControl = myControl)
  
  #Check the model
  model
  plot(model)
}

build_LDA_CV2(5, train_ti, test_ti)

```


Logistic Regression Function
```{r}
buildLR <- function(targets, train_df, test_df, threshold){
  set.seed(123)
  logistic_model = glm(targets~.,data=train_df, family=binomial(link='logit'))
  summary(logistic_model)
  logistic_test <- predict(logistic_model, test_df, type = "response")
  logistic_binary <- ifelse(logistic_test>threshold, 1, 0)
  logistic_error <- errorRate(logistic_binary, test_df[,ncol(test_df)])
  logistic_accuracy <- 1 - logistic_error
  cat("Error Rate:" ,logistic_error,
      "\nAccuracy:  ", logistic_accuracy)
  
  logistic_prediction <- prediction(logistic_test, test_df[,ncol(test_df)])
  roc_logistic = performance(logistic_prediction, measure = "tpr", x.measure = "fpr")
  plot(roc_logistic, main = "ROC Curve for Logistic Regression", colorize = T)
  abline(a=0, b= 1)
  
  auc_logistic <- performance(logistic_prediction, measure = "auc")@y.values[[1]]
  
  cat("\nLogistic Regression AUC:", auc_logistic) 
}
```

```{r}

buildLR(targets, train_ti, test_ti, 0.1)
buildLR(targets, train_fq, test_fq, 0.1)

```



LDA Function

```{r}

library(MASS)

buildLDA <- function(targets, train_df, test_df, threshold){
  lda_model=lda(targets~., data=train_df)
  lda_test <- predict(lda_model, test_df, type = "response")
  lda_prediction <- prediction(lda_test$posterior[,2], test_df[,ncol(test_df)])
  roc_lda <- performance(lda_prediction, measure = "tpr", x.measure = "fpr")
  plot(roc_lda, main = "ROC Curve for LDA", colorize = T)
  abline(a=0, b= 1)
  
  lda_binary <- ifelse(lda_test$posterior[,2]>threshold, 1, 0)
  lda_error <- errorRate(lda_binary, test_df[,ncol(test_df)])
  lda_accuracy <- 1 - lda_error
  
  auc_lda <- performance(lda_prediction, measure = "auc")@y.values[[1]]
  cat("Logistic Regression Error Rate:", 1-lda_accuracy,
      "\nLogistic Regression Accuracy:", lda_accuracy,
      "\nLDA AUC: ", auc_lda)
}

```

```{r}

buildLDA(targets, train_ti, test_ti, 0.1)
buildLDA(targets, train_fq, test_fq, 0.1)

```





